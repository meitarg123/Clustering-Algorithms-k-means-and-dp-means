

אלגוריתם k-means

סעיף 1: תיאור האלגוריתם ומימושו

תחילה, מימשנו את אלגוריתם k-means כך שאתחול k הנקודות (אשר יהוו בסיום ההרצה מרכזי האשכולות השונים) התבצע באופן רנדומלי, בתוך טווח הערכים התחום על ידי ערכי ה-x וה-y המינימליים והמקסימליים מבין הנקודות הנתונות ב- Data set (אותו יש לציין, ייצרנו באמצעות ספריית sklearn ומתודת make\_blobs).

לאחר אתחול k הנקודות (ייקראו מעתה "נקודות מרכזיות"), ביצענו עדכונים חוזרים ונשנים של נקודות אלו לפי האלגוריתם הבא:

1. חישוב המרחק האוקלידי (ייקרא מעתה "מרחק") של כל נקודה בדאטה מכל נקודה מרכזית.
2. הנקודות השונות מסווגות לקבוצות לפי הנקודה המרכזית הקרובה אליהן ביותר.
3. הנקודות המרכזיות מחושבות שוב לפי ממוצע הנקודות ששייכות לכל נקודה מרכזית.

סיום הלולאה מתרחש כשקבוצת הנקודות המרכזיות לא השתנתה בין שתי איטרציות של הלולאה.

על מנת להימנע מלולאה אין-סופית, הוספנו תנאי עצירה נוסף ללולאה המגביל את מספר האיטרציות.

המימוש המלא מצורף כקובץ פייתון נפרד.

סעיף 2: בחינת ביצועי המימוש

על אף הצלחה יחסית, שמנו לב שבכמות ניכרת של איטרציות נצפו ביצועים פחות טובים של האלגוריתם. מתוך הבנה שאתחול הנקודות המרכזיות מתבצע באקראי, הצלחנו לתחקר ולזהות 2 בעיות עיקריות שהביאו לפגיעה בביצועי המימוש:

בגרפים הבאים צבענו בצבע שונה את הנקודות שייצרנו באמצעות make\_blobs, וכל נקודה מיוצגת על ידי **צורה שונה** בהתאם לסיווג k-means.

1. בעיה ראשונה: אם נקודה מרכזית מאותחלת רחוק מהקבוצות שיוצרו על ידי make\_blobs, היא נוטה לא לשנות את מיקומה, ולהתקבע כטעות.

![](RackMultipart20230510-1-x8o9ax_html_21ee57653665d995.png)

סימן הפלוס הימני תחתון הינו נקודה מרכזית בסיום ההרצה, ערכה x=1.292, y=-1.725

מתחקור ההרצה ראינו כי ערכה באתחול היה כמעט זהה: x=1.289, y=-1,724

1. אם שתי נקודות מרכזיות מאותחלות קרוב מדי אחת לשנייה, כנראה שהן לא יתרחקו אחת מהשנייה.

![](RackMultipart20230510-1-x8o9ax_html_9da2a4bf9a20c3.jpg)

סימני ה+ האדומים זה הערכים ההתחלתיים של הנקודות המרכזיות, ואילו ה+ השחורים זה הערכים הסופיים של הנקודות המרכזיות, ואכן ניתן לראות שהנקודות בצד שמאל למטה של התמונה לא התרחקו, וקבוצה אחת (הירוקה) חולקה לשני clusters שונים.

מבדיקת השפעת ערכו של פרמטר k על ביצועי האלגוריתם, הבחנו כי הביצועים של k-means פחות טובים ככל שה-k גדל. זאת, להשערתנו, בשל סיכוי גבוה יותר לאתחול בעייתי של הנקודות המרכזיות, שיוביל לאחת משתי הבעיות שתוארו מעלה. בתמונה המובאת מטה ניתן לראות דוגמא להתקבצות של 3 נקודות מרכזיות שונות שאותחלו בערכים קרובים.

נדגיש - כי אלגוריתם k-means מתאים לבעיות סיווג בהם ידועה כמות הקבוצות השונות מראש, ועל כן - ביצירת הדאטה-סט באמצעות sklearn.datasets.make\_blobs ייצרנו כמות קבוצות בדאטה הזהות לפרמטר k. כנדרש במשימה - פיזרנו 1000 נקודות במרחב דו-מימדי.

![](RackMultipart20230510-1-x8o9ax_html_fa435728627a3536.jpg)

על מנת להתמודד עם בעיות אלה, החלטנו לאתחל את k הנקודות בצורה אחרת:

1. נאתחל את ערכי הנקודה המרכזית הראשונה להיות זהים לאחת מהנקודות הקיימות ב- Data set, נקודה זו נבחרת באקראי.
2. כעת, נאתחל בלולאה את k-1 הנקודות המרכזיות האחרות:
  1. עבור כל נקודה ב Data set, נחשב את סכום המרחקים שלה לבין כל הנקודות המרכזיות שחושבו עד כה. בנוסף, ננרמל מרחק זה למול סכום כל המרחקים של כל הנקודות מכל המרכזים.
  2. כדי לאתחל את ערכי הנקודה המרכזית הבאה, נגריל נקודה אקראית בData set - כשההסתברות להגריל נקודה מסוימת שווה ערך לסכום המרחקים המנורמל שלה.

כעת, ניתן היה לראות שהביצועים טובים יותר. זאת, משום שערכי הנקודה המרכזית ההתחלתיים בהכרח שווים לנקודה כלשהי מה- Data set, ומשום שבהסתברות נמוכה יותר נאתחל שתי נקודות מרכזיות קרוב מדי זו לזו.

אלגוריתם dp-means

סעיף 1: תיאור האלגוריתם ומימושו:

בשונה מה-kmeans, באלגוריתם זה מספר הclusters להם מחולק הדאטה לא ידוע לנו מראש - ולכן הוא מתאים לסוג בעיות נוספות. האלגוריתם מוצא את מספר ה-clusters במהלך הריצה, תוך התחשבות בפרמטר λ, באופן הבא:

מאתחלים נקודה מרכזית ראשונה יחידה על ידי הממוצע של כל הנקודות בדאטה סט. ומתחילים להריץ בלולאה:

בכל איטרציה בלולאה, עוברים על כל הנקודות בדאטה-סט: עבור כל נקודה בדאטה-סט, מחשבים את מרחקה מכל נקודה מרכזית ומוצאים את המרחק המינימלי עבורה. כעת, נבדוק האם מרחק מינימלי זה גדול מ- λ. במידה שכן - ניצור נקודה מרכזית חדשה, שערכיה שווים לנקודה אותה בדקנו. נמשיך לעבור על שאר הנקודות כשבסיום איטרציה, בדומה לk-means, כל נקודה "משויכת" לנקודה מרכזית מסוימת.

בדומה לk-means, בכל איטרציה, לאחר מעבר על כל הנקודות בדאטה-סט באיטרציה, מתבצע מיטוב של הנקודות המרכזיות לפי ממוצע הנקודות ה"שייכות" אליהן בדאטה-סט.

אם לא היה שינוי בערך הנקודות המרכזיות או בכמותן בין שתי איטרציות, האלגוריתם יחזור לבצע איטרציה נוספת של הלולאה.

סעיף 2: בחינת ביצועי המימוש

הערך λ מייצג את המרחק האוקלידי המקסימלי של נקודה בדאטה-סט מהנקודה המרכזית הקרובה ביותר אליה. על כן, ערך λ קטן מדי (למשל, קטן ממחצית המרחק בין כל 2 נקודות) יביא לכך (במימוש שלנו, בהעדר הגבלה על כמות הנקודות המרכזיות) שכל נקודה תהפוך נקודה מרכזית.

בתמונה: ערך λ הוא 0.001

![](RackMultipart20230510-1-x8o9ax_html_6781112305e4a819.png)

מצד שני, ערך λ גדול מדי יביא לכך שלא יווצרו נקודות מרכזיות חדשות כלל, וכל הנקודות יסווגו לנקודה מרכזית אחת.

בתמונה: ערך λ הוא 2.4

![](RackMultipart20230510-1-x8o9ax_html_f9b3bc9c77f206e4.png)

על כן, ישנו אופטימום כלשהו בין 2 ערכי קיצון אלה. מבחינת ערכי λ הבחנו כי הערך האופטימלי של λ תלוי בכמה פרמטרים:

1. מספר הקבוצות בדאטה-סט: הערך של λ משפיע על כמות הנקודות המרכזיות שנוצרות, ערך λ גדול יביא ליצירת פחות נקודות מרכזיות, כשערך λ קטן יביא לתוצאה הפוכה. על כן, בהינתן מספר קטן של קבוצות בדאטה-סט - ערכי λ גדולים הביאו תוצאות טובות יותר.

בתמונה המצורפת ערכו של λ הוא 1, ו- 1000 הנקודות מפוזרות כ-2 קבוצות. ה "+" מייצגים את הנקודות המרכזיות בסיום ריצת האלגוריתם.

![](RackMultipart20230510-1-x8o9ax_html_11046798db83c401.png)

ובתמונה זו ערכו של λ הוא 2: כאמור, ניתן לראות סיווג נכון יותר של הדאטה-סט clusters

![](RackMultipart20230510-1-x8o9ax_html_400fc41c23430fa5.png)

1. פיזור הנקודות במרחב: בקלאסטרים "גדולים" (בעלי רדיוס גדול) הגדלת ערך λ הביאה למיטוב נקודה מרכזית למרכז הקלאסטר, בעוד שהקטנת ערך λ הביאה ליצירת נקודות מרכזיות נוספות לאותו קלאסטר.

בתמונה המובאת מטה ערכו של λ הינו 1.2, כשפיזרנו 4 קבוצות של נקודות במרחב (לפי צבעים), האלגוריתם טעה בכך שסיווג חלק מהקבוצות ל-2 קלאסטרים שונים.

![](RackMultipart20230510-1-x8o9ax_html_576199b195134bdb.png)

זאת, לעומת ערך גדול יותר של λ (1.7), שהביאה לביצועים טובים יותר:

![](RackMultipart20230510-1-x8o9ax_html_dc87a363bf307f53.png)

**באופן כללי**** , **** הקטנת **** λ **** גורמת ל ****"**** העדפת ****"**  **יצירת נקודות מרכזיות חדשות**** , **** והגדלתו להתאמת הנקודות המרכזיות הקיימות ****.**

סעיף 3:

Kmeans: מבחינת אלגוריתם Kmeans על תמונת קוף מדריל (512\*512), ניתן לראות (בתמונה המובאת מטה) שהגדלת ערכו של K הביאה לביצועים טובים יותר. ![](RackMultipart20230510-1-x8o9ax_html_15160cc2e952950.png)

עבור ערכי k גבוהים (30 ואילך), ראינו שאין שינוי משמעותי בביצועי האלגוריתם. ![](RackMultipart20230510-1-x8o9ax_html_c6de4c01e2be3981.png)

נציין כי על אף שבסעיף 1 טענו שהגדלת ערכי k מביאים לירידה בביצועי האלגוריתם, שתי הטעויות המרכזיות שאפיינו לאלגוריתם בסעיף 1:

- התקבצות נקודות מרכזיות באותה קבוצת נקודות קרובות בדאטה-סט וחלוקה (מיותרת) שלהן לקלאסטרים שונים.
- התמקמות נקודה מרכזית רחוק מכל נקודה בדאטה-סט.

לא משפיעות על ביצועי האלגוריתם במשימת צביעת קוף המנדריל. זאת, משום שקרבה בין שתי נקודות מרכזיות תביא לצביעה זהה (או דומה מספיק) של תמונת הפלט. כמו כן, נקודה מרכזית שמוקמה רחוק מכל נקודה בדאטה-סט, לא תביא לצביעה שגויה משמעותית (אם בכלל) של פיקסלים בתמונת הפלט.

זמן הריצה: ערכי K גבוהים הביאו כצפוי לזמני ריצה ארוכים יותר. עבור 4 התמונות הראשונות זמן הריצה היה 9.4 שניות. בעבור 4 התמונות האחרונות זמן הריצה היה 27.35 שניות.

DPMeans:מבחינת אלגוריתם DPMeans ראינו כי הורדת ערכה של λ הביאה לביצועים טובים יותר של האלגוריתם.

זאת, משום שהורדת ערכה של λ, מאפשר לאלגוריתם ליצור יותר נקודות מרכזיות ובכך להצליח לסווג יותר נקודות בדאטה-סט ליותר קלאסטרים בהתאם לצורך.

ערך λ גבוה מדי יביא ליצירת מעט נקודות מרכזיות, ועל כן לסיווג הנקודות בדאטה-סט למספר מועט של clusters - מה שיביא לתמונת פלט פחות מגוונת (כפי שניתן לראות בתמונה המובאת מטה)

![](RackMultipart20230510-1-x8o9ax_html_2dedc2541daaf8e2.png)

הורדת ערכו של λ מביאה ליותר איטרציות (יצירה של יותר נקודות מרכזיות, ומיטובן) ועל כן כצפוי הורדת ערכו של λ הביאה לעלייה בזמן הריצה. אולם - זמן הריצה לא עלה באופן משמעותי.

ערך λ נמוך מספיק מביא ליצירת נקודה מרכזית עבור כל נקודה בדאטה-סט (כפי שתיארנו בסעיפים 1-2), כך שאין שוני מהתמונה המקורית).

![](RackMultipart20230510-1-x8o9ax_html_723e2925ce1c174.png)

סעיף 4: עבור הפרויקט הסופי שלנו בחרנו בפרויקט המוצע, מספר 1:

Project #1: Extend the (P)DC-DP-means Algorithm to Streaming data
